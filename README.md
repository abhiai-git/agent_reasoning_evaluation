# agent_reasoning_evaluation
trajectory-eval is a Python package designed to evaluate the reasoning trajectories of tool-enabled LLM agents, Traditional evaluation methods often focus solely on final-answer accuracy, overlooking how an agent arrives at its conclusions. trajectory-eval addresses this by analyzing the entire reasoning process â€” step-by-step 
